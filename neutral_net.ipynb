{
  "cells": [
    {
      "cell_type": "code",
      "source": "import numpy as np",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Implementation of Perceptron and Backpropagation algorithm \n## Perceptron",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "class Perceptron:\n    def __init__(self, N, alpha\u003d0.1):\n        # Initialise the weight matrix and store the learning rate\n        self.W \u003d np.random.randn(N + 1) / np.sqrt(N)\n        self.alpha \u003d alpha\n\n    @staticmethod\n    def step(self, x):\n        # Apply the step function\n        return 1 if x \u003e 0 else 0\n\n    def fit(self, X, y, epochs\u003d10):\n        # The X value is our actual training data\n        # y variable is our target output class labels\n\n        # Insert a column of 1\u0027s as the last entry of the feature matrix\n        # This allows us the treat the bias as a trainable parameter with the\n        #  weight matrix\n        X \u003d np.c_[X, np.ones((X.shape[0]))]\n\n        # Loop over the desired number of epochs\n        for epoch in np.arange(0, epochs):\n            # Loop over each individual data point\n            for (x, target) in zip(X, y):\n                # Pass the dot product of the input features and weight matrix through the step function\n                pred \u003d self.step(np.dot(x, self.W))\n\n                # Only performs a weight update if our prediction does not match the target\n                if pred !\u003d target:\n                    # Calculate the error\n                    error \u003d pred - target\n\n                    # Update the weight matrix\n                    self.W +\u003d -self.alpha * error * x\n\n    def predict(self, X, add_bias\u003dTrue):\n\n        # Ensure our input is a matrix\n        X \u003d np.atleast_2d(X)\n\n        # Check to see if the bias column should be added\n        if add_bias:\n            # Insert a column of 1\u0027s as the last entry in the feature matrix\n            X \u003d np.c_[X, np.ones((X.shape[0]))]\n\n        # Pass the dot product of the input features and weight matrix through the step function\n        return self.step(np.dot(X, self.W))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Test Perceptron with XOR problem",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "source": "from utilities.nn import Perceptron\nimport numpy as np\n\n# Construct the \u0027XOR\u0027 dataset\nX \u003d np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\ny \u003d np.array([[0], [1], [1], [0]])\n\n# Train the perceptron\nprint(\u0027[INFO]: Training....\u0027)\np \u003d Perceptron(X.shape[1], alpha\u003d0.1)\np.fit(X, y, epochs\u003d20)\n\n# Test the perceptron\nprint(\u0027[INFO]: Testing....\u0027)\n\n# Loop over the data points\nfor (x, target) in zip(X, y):\n    # Make a prediction and display the result\n    pred \u003d p.predict(x)\n    print(\u0027[INFO]: Data\u003d{}, Ground Truth\u003d{}, Prediction\u003d{}\u0027.format(\n        x, target[0], pred))\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Backpropagation (NeuralNetwork)",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "class NeuralNetwork:\n    def __init__(self, layers, alpha\u003d0.1):\n        # Initialise the list of weight matrices, network architecture and learning rate\n        self.W \u003d []\n        self.layers \u003d layers  # [2,2,1]\n        self.alpha \u003d alpha\n\n        # Start looping from the index of the first layer but stop before we reach the last 2 layers\n        for i in np.arange(0, len(layers) - 2):\n            # Randomy initialise a weight matrix connecting the number of nodes in each respective layer together,\n            # adding an extra node for the bias\n            w \u003d np.random.randn(layers[i] + 1, layers[i + 1] + 1)\n            self.W.append(w / np.sqrt(layers[i]))\n\n        # The last 2 layers are a special case where the input connections need a bias term but the output does not\n        w \u003d np.random.randn(layers[-2] + 1, layers[-1])\n        self.W.append(w / np.sqrt(layers[-2]))\n\n    def __repr__(self):\n        # Return string that represents the network architecture\n        return \u0027Neural Network: {}\u0027.format(\u0027-\u0027.join(\n            str(l) for l in self.layers))\n\n    def sigmoid(self, x):\n        # Compute the sigmoid activation value\n        return 1.0 / (1 + np.exp(-x))\n\n    def sigmoid_deriv(self, x):\n        # Compute the derivative of the sigmoid function assuming that \u0027x\u0027 has already been passed through the\n        # sigmoid function\n        return x * (1 - x)\n\n    def fit(self, X, y, epochs\u003d1000, display_update\u003d100):\n        # Insert a column of 1\u0027s as the last entry of the feature matrix. This allows us the treat the bias as a\n        # trainable parameter with the weight matrix\n        X \u003d np.c_[X, np.ones((X.shape[0]))]\n\n        # Loop over the number of epochs\n        for epoch in np.arange(0, epochs):\n            # Loop over each data point and train the network on it\n            for (x, target) in zip(X, y):\n                self.fit_partial(x, target)\n\n            # Check to see if we should display a training update\n            if epoch \u003d\u003d 0 or (epoch + 1) % display_update \u003d\u003d 0:\n                loss \u003d self.calculate_loss(X, y)\n                print(\u0027[INFO]: epoch\u003d{}, loss\u003d{:.5f}\u0027.format(epoch + 1, loss))\n\n    def fit_partial(self, x, y):\n        # Construct list of output activities for each layer as the data point flows through the network. The first\n        # layer is just the input feature vector itself\n        A \u003d [np.atleast_2d(x)]\n\n        # FEED-FORWARD:\n        # loop over the layers in the network\n        for layer in np.arange(0, len(self.W)):\n            # Feed forward the activation at the current layer by taking the dot product of the activation and the\n            # weight matrix - called the \u0027net input\u0027 to the current layer\n            net \u003d A[layer].dot(self.W[layer])\n\n            # The \u0027net output\u0027 is simply applying the sigmoid function to the net input\n            out \u003d self.sigmoid(net)\n\n            # Add the net output to the list of activations\n            A.append(out)\n\n        # BACK-PROPAGATION:\n        # Compute the difference between the \u0027prediction\u0027 (final net output in the activation list) and the true\n        # target value\n        error \u003d A[-1] - y\n\n        # Apply the chain rule to build a list of deltas. The first entry is simply the error of the output layer\n        # times the derivative of the activation function for the ouput value\n        D \u003d [error * self.sigmoid_deriv(A[-1])]\n\n        # Loop over the layers in reverse order (ignoring the last 2 layers)\n        for layer in np.arange(len(A) - 2, 0, -1):\n            # The delta for the current layer is equal to the delta of the \u0027previous layers\u0027 dotted with the weight\n            # matrix of the current layer, followed by multiplying the delta by the derivative of the activation\n            # function for the activations of the current layer\n            delta \u003d D[-1].dot(self.W[layer].T)\n            delta \u003d delta * self.sigmoid_deriv(A[layer])\n            D.append(delta)\n\n        # Since we looped over the layer in reverse order we need to reverse the deltas\n        D \u003d D[::-1]\n\n        # WEIGHT-UPDATE-PHASE:\n        # Loop over the layers\n        for layer in np.arange(0, len(self.W)):\n            # Update the weights by taking the dot product of the layer activations with their respective deltas,\n            # then multiplying this value by the learning rate and adding to the weight matrix\n            self.W[layer] +\u003d -self.alpha * A[layer].T.dot(D[layer])\n\n    def predict(self, X, add_bias\u003dTrue):\n        # Initialise the output prediction as the input features. This value will be (forward) propagated through the\n        # network to obtain the final prediction\n        p \u003d np.atleast_2d(X)\n\n        # Check to see if the bias column should be added\n        if add_bias:\n            # Insert a column of 1\u0027s as the last entry in the feature matrix\n            p \u003d np.c_[p, np.ones((p.shape[0]))]\n\n        # Loop over the layers in the network\n        for layer in np.arange(0, len(self.W)):\n            # Compute the output prediction\n            p \u003d self.sigmoid(np.dot(p, self.W[layer]))\n\n        # Return the predicted value\n        return p\n\n    def calculate_loss(self, X, targets):\n        # Make predictions for the input data points then compute the loss\n        targets \u003d np.atleast_2d(targets)\n        predictions \u003d self.predict(X, add_bias\u003dFalse)\n        loss \u003d 0.5 * np.sum((predictions - targets)**2)\n\n        # Return the loss\n        return loss",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Test NeuralNetwork with MINST",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "from utilities.nn import NeuralNetwork\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn import datasets\n\n# Load the MNIST dataset and apply min/max scaling to scale the pixel intensity values to the range [0, 1] (each\n# image is represented as an 8x8 \u003d 64-dim feature vector\ndigits \u003d datasets.load_digits()\ndata \u003d digits.data.astype(\u0027float\u0027)\ndata \u003d (data - data.min()) / (data.max() - data.min())\nprint(\u0027[INFO]: Samples\u003d{}, Dimension\u003d{}\u0027.format(data.shape[0], data.shape[1]))\n\n# Construct the training and testing splits\n(train_x, test_x, train_y, test_y) \u003d train_test_split(\n    data, digits.target, test_size\u003d0.25)\n\n# Convert the labels from integers to vectors\ntrain_y \u003d LabelBinarizer().fit_transform(train_y)\ntest_y \u003d LabelBinarizer().fit_transform(test_y)\n\n# Train the network\nprint(\u0027[INFO]: Training....\u0027)\nnn \u003d NeuralNetwork([train_x.shape[1], 32, 16, 10])\nprint(\u0027[INFO]: {}\u0027.format(nn))\nnn.fit(train_x, train_y, epochs\u003d1000)\n\n# Test the network\nprint(\u0027[INFO]: Testing....\u0027)\npredictions \u003d nn.predict(test_x)\npredictions \u003d predictions.argmax(axis\u003d1)\nprint(classification_report(test_y.argmax(axis\u003d1), predictions))\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "stem_cell": {
      "cell_type": "raw",
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}