{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Deep learning in computer vision\n## 1. Image and Classification Fundamentals: \n\nFour steps in the deep learning classification pipeline:\n    - Gathering our dataset\n    - Splitting our data into training, testing, and validation steps, \n    - Training our network,\n    - Finally evaluating our model.",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## 2. Parameterized:\n\nComponents of parameterized learning:\n    1. Data: - In the context of image classification, our input data is our dataset of images. \n    2. Scoring function: The scoring function produces predictions for a given input image.\n    3. Loss function: The loss function then quantifies how good or bad a set of predictions are over the dataset.\n    4. Weights and biases: The weight matrix (W) and bias (b) vectors are what enable us to actually “learn” from the input data – these parameters will be tweaked and tuned via optimization methods in an attempt to obtain higher classification accuracy.\n\nHinge loss and cross-entropy loss:\n- Hinge loss: $L_i \u003d \\sum_{j \\neq y_i} max(0,s_i - s_{y_i} + 1) $\n- Cross-entropy loss: $L_i \u003d  -log(e^{s_{y_i}}/ \\sum_{j}e^{s_j}) $\n\nwhere ${s_j} $ predicted score of the j-th class via the i-th data point:\n    ${s_j \u003d f(x_i, W)}$\n## 3. Optimization Methods:\n\n  - Most important aspect of machine learning, neural networks, and deep learning is optimization.\n\n#### Gradient descent: (Optimization Methods):\n    Gradient descent algorithms are controlled via a learning rate:\n    There are two types of gradient descent:\n    1. The standard vanilla flavor: Vanilla gradient descent performs only one weight update per epoch,\n     making it very slow (if not impossible) to converge on large datasets.\n    \n    2. The stochastic version that is more commonly used: since it applies multiple weight updates per epoch by computing the gradient on small mini-batches.\n    \n        By using SGD we can dramatically reduce the time it takes to train a model while also enjoying lower loss and higher accuracy.\n\nPseudocode for Gradient Descent ( standard vanilla flavor version)",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "while True:\n    W_gradient \u003d evaluate_gradient(loss, data, W)\n    W +\u003d -alpha * W_gradient\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "1. Looping until some condition is met, typical are: \n    + Specified number of epochs has passed.\n    + Our loss has become sufficiently low or training accuracy satisfactory high.\n    + Or loss has not improved in M subsequent epochs.\n2. Then calls a function named evaluate_gradient. \nThis function requires three parameters:\n    1. loss: A function used to compute the loss over our current W and input data.\n    2. data: Our training data where each training sample is represented by an image.\n    3. W: Our actual weight matrix that we are optimizing over.\n    Our goal is to apply gradient descent to find a W that yields minimal loss.\n3. We then apply gradient descent. We multiply our W_gradient by alpha (a), our learning rate.\n    The learning rate controls the size of our step.",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Pseudocode for Gradient Descent ( SGD version)",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "while True:\n    batch \u003d next_training_batch(data, 256)\n    W_gradient \u003d evaluate_gradient(loss, batch, W)\n    W +\u003d -alpha * W_gradient",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "The only difference between vanilla gradient descent and SGD is the addition of\nthe next_training_batch function.\n\nInstead of computing our gradient over the entire data set, we instead sample our data,\nyielding a batch. We evaluate the gradient on the batch, and update our weight matrix W.\nWe also try to randomize our training samples before applying SGD since the algorithm is\nsensitive to batches\n\nTypical batch sizes include 32, 64, 128 and 256\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## 4. Regularization\nRegularization helps us control our model capacity, ensuring that our models are better at\nmaking (correct) classifications on data points that they were not trained on, which we call the\nability to generalize\n\nThree common types of regularization there are applied directly to the loss function.\n- L2 regularization (“weight decay”): \n\n- L1 regularization which takes the absolute value rather than the square:\n\n- Elastic Net regularization seeks to combine both L1 and L2 regularization:\n\nIn deep learning and neural networks,  the L2 regularization used commonly for image classification \n– the trick is tuning the alpha parameter to include just the right amount of regularization.\n\n## 5. Neural Network (artificial)\nImplement with keras: Link to [neutral_net](neutral_net.ipynb)\n### Perceptron architecture:\n![Perceptron](images/Selection_003.png)\n### Perceptron Training Procedure \n1. Initialize our weight vector w with small random values\n2. Until Perceptron converges:\n    - Loop over each feature vector $x_j$ and true class label $d_i$ in our training set D\n    - Take x and pass it through the network, calculating the output value: $y_j \u003d f(w(t)·xj)$\n    - Update the weights w: ${w}_i (t +1) \u003d w_i(t)+ \\alpha(d_j − y_j)x_{ji}$  for all features $0 \u003c\u003d i \u003c\u003d n$\n\n### Multi-layer Networks:\nBackpropagation is the most important algorithm in neural network: Backpropagation can be considered\nthe cornerstone of modern neural networks and deep learning.\n1. The forward pass where our inputs are passed through the network and output predictions obtained.\n2. The backward pass where we compute the gradient of the loss function at the final layer (i.e.,\npredictions layer) of the network and use this gradient to recursively apply the chain rule to\nupdate the weights in our network.\n(Backpropagation: efficiently train neural networks and “teach” them to learn from their mistakes.)\n\n\n### Neutral Network Recipe:\n- Dataset\n- Loss Function (\u0027categorical cross-entropy\u0027) (\u0027binary cross-entropy\u0027)\n- Model/Architecture: \n    1. How many data points you have.\n    2. The number of classes.\n    3. How similar/dissimilar the classes are.\n    4. The intra-class variance.\n- Optimization Method: SGD (Stochastic Gradient Descent)\n\n\n## 6. Convolutional Neural Networks (CNNs)\n### Convolutions: Link to [convolutions](convolutions.ipynb)\n### CNN Building Blocks:Link to [convolutions](cnn_building_block.ipynb)\n\n\n## 7. Learning Rate Schedulers\n## 8. Underfitting and Overfitting\n## 9. Checkpointing Models\n## 10. Architecture Visualization\n## 11. The (Mini) VGGNet Architecture\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "stem_cell": {
      "cell_type": "raw",
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}