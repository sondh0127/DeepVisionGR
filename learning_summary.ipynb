{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Work-item 2:\nHọc Deep learning in computer vision.\nKeras Framework\n## 1. Image and Classification Fundamentals: \n\nFour steps in the deep learning classification pipeline:\n- Gathering our dataset\n- Splitting our data into training, testing, and validation steps, \n- Training our network,\n- Finally evaluating our model.",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## 2. Parameterized:\n\nComponents of parameterized learning:\n1. Data: - In the context of image classification, our input data is our dataset of images. \n2. Scoring function: The scoring function produces predictions for a given input image.\n3. Loss function: The loss function then quantifies how good or bad a set of predictions are over the dataset.\n4. Weights and biases: The weight matrix (W) and bias (b) vectors are what enable us to actually “learn” from the input data – these parameters will be tweaked and tuned via optimization methods in an attempt to obtain higher classification accuracy.\n\nHinge loss and cross-entropy loss:\n- Hinge loss: $L_i \u003d \\sum_{j \\neq y_i} max(0,s_i - s_{y_i} + 1) $\n- Cross-entropy loss: $L_i \u003d  -log(e^{s_{y_i}}/ \\sum_{j}e^{s_j}) $\n\nwhere ${s_j} $ predicted score of the j-th class via the i-th data point:\n    ${s_j \u003d f(x_i, W)}$\n## 3. Optimization Methods:\n\n  - Most important aspect of machine learning, neural networks, and deep learning is optimization.\n\n#### Gradient descent: (Optimization Methods):\nGradient descent algorithms are controlled via a learning rate:\nThere are two types of gradient descent:\n1. The standard vanilla flavor: Vanilla gradient descent performs only one weight update per epoch,\n making it very slow (if not impossible) to converge on large datasets.\n\n2. The stochastic version that is more commonly used: since it applies multiple weight updates per epoch by computing the gradient on small mini-batches.\n\n    By using SGD we can dramatically reduce the time it takes to train a model while also enjoying lower loss and higher accuracy.\n\nPseudocode for Gradient Descent ( standard vanilla flavor version)",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "while True:\n    W_gradient \u003d evaluate_gradient(loss, data, W)\n    W +\u003d -alpha * W_gradient\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "1. Looping until some condition is met, typical are: \n    + Specified number of epochs has passed.\n    + Our loss has become sufficiently low or training accuracy satisfactory high.\n    + Or loss has not improved in M subsequent epochs.\n2. Then calls a function named evaluate_gradient. \nThis function requires three parameters:\n    1. loss: A function used to compute the loss over our current W and input data.\n    2. data: Our training data where each training sample is represented by an image.\n    3. W: Our actual weight matrix that we are optimizing over.\n    Our goal is to apply gradient descent to find a W that yields minimal loss.\n3. We then apply gradient descent. We multiply our W_gradient by alpha (a), our learning rate.\n    The learning rate controls the size of our step.",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Pseudocode for Gradient Descent ( SGD version)",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "while True:\n    batch \u003d next_training_batch(data, 256)\n    W_gradient \u003d evaluate_gradient(loss, batch, W)\n    W +\u003d -alpha * W_gradient",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "The only difference between vanilla gradient descent and SGD is the addition of\nthe next_training_batch function.\n\nInstead of computing our gradient over the entire data set, we instead sample our data,\nyielding a batch. We evaluate the gradient on the batch, and update our weight matrix W.\nWe also try to randomize our training samples before applying SGD since the algorithm is\nsensitive to batches\n\nTypical batch sizes include 32, 64, 128 and 256\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## 4. Regularization\nRegularization helps us control our model capacity, ensuring that our models are better at\nmaking (correct) classifications on data points that they were not trained on, which we call the\nability to generalize\n\nThree common types of regularization there are applied directly to the loss function.\n- L2 regularization (“weight decay”): \n\n- L1 regularization which takes the absolute value rather than the square:\n\n- Elastic Net regularization seeks to combine both L1 and L2 regularization:\n\nIn deep learning and neural networks,  the L2 regularization used commonly for image classification \n– the trick is tuning the alpha parameter to include just the right amount of regularization.\n\n## 5. Neural Network (artificial)\nImplement with keras: Link to [neutral_net](neutral_net.ipynb)\n### Perceptron architecture:\n![Perceptron](images/Selection_003.png)\n### Perceptron Training Procedure \n1. Initialize our weight vector w with small random values\n2. Until Perceptron converges:\n    - Loop over each feature vector $x_j$ and true class label $d_i$ in our training set D\n    - Take x and pass it through the network, calculating the output value: $y_j \u003d f(w(t)·xj)$\n    - Update the weights w: ${w}_i (t +1) \u003d w_i(t)+ \\alpha(d_j − y_j)x_{ji}$  for all features $0 \u003c\u003d i \u003c\u003d n$\n\n### Multi-layer Networks:\nBackpropagation is the most important algorithm in neural network: Backpropagation can be considered\nthe cornerstone of modern neural networks and deep learning.\n1. The forward pass where our inputs are passed through the network and output predictions obtained.\n2. The backward pass where we compute the gradient of the loss function at the final layer (i.e.,\npredictions layer) of the network and use this gradient to recursively apply the chain rule to\nupdate the weights in our network.\n(Backpropagation: efficiently train neural networks and “teach” them to learn from their mistakes.)\n\n\n### Neutral Network Recipe:\n- Dataset\n- Loss Function (\u0027categorical cross-entropy\u0027) (\u0027binary cross-entropy\u0027)\n- Model/Architecture: \n1. How many data points you have.\n2. The number of classes.\n3. How similar/dissimilar the classes are.\n4. The intra-class variance.\n- Optimization Method: SGD (Stochastic Gradient Descent)\n\n\n## 6. Convolutional Neural Networks (CNNs)\n### Convolutions: Link to [convolutions](convolutions.ipynb)\n### CNN Building Blocks:Link to [convolutions](cnn_building_block.ipynb)\n\n\n# Work-item 4:\nDeep learning Framework: tìm hiểu thêm về keras and tensorflow. \n## 7. Learning Rate Schedulers\n- Learning Rate Schedulers used to increase classification accuracy.\n- Two primary types of learning rate schedulers:\n    1. Time-based schedulers that gradually decrease based on epoch number.\n    2. Drop-based schedulers that drop based on a specific epoch, similar to the behavior of a piecewise function\n\nStandard time-based schedule provided by Keras (with the rule of thumb of decay \u003d alpha_init / epochs):\n```python\nopt \u003d SGD(lr\u003d0.01, decay\u003d0.01 / 40, momentum\u003d0.9, nesterov\u003dTrue)\n```\nKeras applies learning rate schedule to adjust the learning rate after every epoch.\nWe need to change learning rate through many experiments to obtain a high accuracy model.\n\n## 8. Underfitting and Overfitting\nUnderfitting occurs when your model cannot obtain sufficiently low loss on the training set.\nIn this case, ours model fails to learn the underlying patterns in your training data.\nOn the other end of the spectrum, we have overfitting where your network models the training data\ntoo well and fails to generalize to your validation data.\nTherefore, our goal when training a machine learning model is to:\n    1. Reduce the training loss as much as possible.\n    2. While ensuring the gap between the training and testing loss is reasonably small.\n\nControlling whether a model is likely to underfit or overfit can be accomplished by adjusting\nthe capacity of the neural network.\nWe can increase capacity by adding more layers and neurons to\nour network. Similarly, we can decrease capacity by removing layers and neurons and applying\nregularization techniques (weight decay, dropout, data augmentation, early stopping, etc.).\n\nUnderfitting is relatively easy to combat: simply add more layers/neurons to your network.\nOverfitting is an entirely different beast though. When overfitting occurs you should consider:\n    1. Reducing the capacity of your network by removing layers/neurons (not recommended unless\n    for small dataset).\n    2. Applying stronger regularization techniques.\n\n## 9. Checkpointing Models\nWe can monitor a given metric (e.x., validation loss, validation accuracy,\netc.) during training and then save high performing networks to disk.\nThere are two methods to accomplish this inside Keras:\n1. Checkpoint incremental improvements.\n2. Checkpoint only the best model found during the process.\n\n```python\n# construct the callback to save only the *best* model to disk\n# based on the validation loss\ncheckpoint \u003d ModelCheckpoint(args[\"weights\"], monitor\u003d\"val_loss\", save_best_only\u003dTrue, verbose\u003d1)\ncallbacks \u003d [checkpoint]\n# train the network\nprint(\"[INFO] training network...\")\nH \u003d model.fit(trainX, trainY, validation_data\u003d(testX, testY),\nbatch_size\u003d64, epochs\u003d40, callbacks\u003dcallbacks, verbose\u003d2)\n```\n## 10. Architecture Visualization\nThe process of constructing a graph of nodes and associated connections in a network\nand saving the graph to disk as an image\n\nThese graphs typically include the following components for each layer:\n1. The input volume size.\n2. The output volume size.\n3. And optionally the name of the layer.\n\nWe typically use network architecture visualization when (1) debugging our own custom\nnetwork architectures and (2) publication, where a visualization of the architecture is easier to\nunderstand than including the actual source code or trying to construct a table to convey the same\ninformation.\n\nSample of LeNet network architecture visualization with keras\n```python\n# import the necessary packages\nfrom pyimagesearch.nn.conv import LeNet\nfrom keras.utils import plot_model\n\n# initialize LeNet and then write the network architecture\n# visualization graph to disk\nmodel \u003d LeNet.build(28, 28, 1, 10)\nplot_model(model, to_file\u003d\"lenet.png\", show_shapes\u003dTrue)\n```\n## 11. State-of-the-art CNNs in Keras\nKeras library ships with many CNNs that have been pre-trained on the ImageNet dataset:\n* VGG16\n* VGG19\n* ResNet50\n* Inception V3\n* Xception\nDepending on our own motivation and end goals of studying deep learning, these networks alone may be enough to build\nown desired application.\n\n## 12. Data Augmentation\n\nAccording to Goodfellow et al., regularization is “any modification we make to a learning algorithm that is intended\nto reduce its generalization error, but not its training error”\n![Perceptron](images/Selection_007.png)\n* Data augmentation is a type of regularization technique that operates on the training data.\n* Data augmentation randomly modify our training data by applying a series of random\ntranslations, rotations, shears, and flips.\n* Applying these simple transformations does not change the class label of the input image; however, each augmented\nimage can be considered a “new” image that the training algorithm has not seen before.\n* Therefore, our training algorithm is being constantly presented with new training samples, allowing it to learn\n more robust and discriminative patterns.\n \nSimple of data augmentaion in keras\n ```python\nfrom keras.preprocessing.image import ImageDataGenerator\naug \u003d ImageDataGenerator(rotation_range\u003d30, width_shift_range\u003d0.1, \n    height_shift_range\u003d0.1, shear_range\u003d0.2, zoom_range\u003d0.2,\n    horizontal_flip\u003dTrue, fill_mode\u003d\"nearest\")\n# construct the actual Python generator\nimageGen \u003d aug.flow(image, batch_size\u003d1, save_to_dir\u003dargs[\"output\"],\n    save_prefix\u003dargs[\"prefix\"], save_format\u003d\"jpg\")\n ```\n\n## 13. Transfer learning\nTransfer learning is the concept of using a pre-trained Convolutional Neural Network to classify class labels outside of what it was originally trained on. In\ngeneral, there are two methods to perform transfer learning when applied to deep learning and\ncomputer vision:\n### 13.1: Networks as Feature Extractors\nTreat networks as feature extractors, forward propagating the image until a given layer, and\nthen taking these activations and treating them as feature vectors.\n\n * Deep CNNs such as VGG, Inception, and ResNet are capable of acting as powerful feature extraction machines,\neven more powerful than hand-designed algorithms such as HOG, SIFT, and Local Binary Patterns.\n\n### 13.2. Fine-tuning Networks\nFine-tuning networks by adding a brand-new set of fully-connected layers to the head of\nthe network and tuning these FC layers to recognize new classes (while still using the same\nunderlying CONV filters). The layers in the body of the original network are frozen while we train the new FC layers.\n\nApplying fine-tuning is an extremely powerful technique as we do not have to train an entire network from scratch.\nInstead, we can leverage pre-existing network architectures, such as state-of-the-art models trained on the ImageNet\ndataset which consist of a rich, discriminative set of filters. \n\nUsing these filters, we can “jump start” our learning, allowing us to perform network\nsurgery, which ultimately leads to a higher accuracy transfer learning model with less effort (and headache)\nthan training from scratch.\n\n## 14.  Single Shot Detectors (SSDs)\nThe SSD object detector is entirely end-to-end, contains no complex moving parts,\n and is capable of super real-time performance. \n\nThe SSD composes of 2 parts:\nExtract feature maps, and Apply convolution filters to detect objects.\nThe SSD starts with a base network (typically pre-trained network).\nA set of new CONV layers are used to replace later CONV and POOL layers.\nEach CONV layer connects to the output FC layer.\nCombined with the (modified) Multibox algorithm, this allows SSDs to detect objects at varying\nscales in the image in a single forward pass.\n\n### TODO: \n## 15. Faster R-CNNs\n## 16. Advanced Optimization Methods\nWorking with HDF5 and Large Datasets, \nWorking with one of popular network like GoogLeNet, MobileNet, ResNet\nWorking with ImageNet dataset\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "stem_cell": {
      "cell_type": "raw",
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}